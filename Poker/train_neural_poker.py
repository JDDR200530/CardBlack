#!/usr/bin/env python3
"""
Script de entrenamiento avanzado para el agente neural de poker
Integra la red neuronal profunda con el entorno de poker existente
"""

import sys
import os
import time
import matplotlib.pyplot as plt
import numpy as np

# Importar mÃ³dulos del proyecto
from holdem_env import HoldemEnv
from kuhn_env import KuhnEnv
from neural_agent import NeuralPokerAgent, PokerNeuralTrainer
from poker_agent import HeuristicAgent, RandomAgent, ConservativeAgent, AggressiveAgent


def entrenar_neural_holdem(n_episodios=2000, n_jugadores=4, guardar_cada=200):
    """
    Entrenamiento del agente neural en Texas Hold'em
    """
    print("ğŸ°" + "="*60 + "ğŸ°")
    print("    ENTRENAMIENTO NEURAL - TEXAS HOLD'EM")
    print("ğŸ°" + "="*60 + "ğŸ°")
    
    # Crear entorno
    env = HoldemEnv(n_players=n_jugadores)
    print(f"ğŸ® Entorno creado: {n_jugadores} jugadores")
    
    # Crear agente neural
    agent = NeuralPokerAgent(
        input_size=20,  # TamaÃ±o correcto que genera obs_to_vector_enhanced
        hidden_sizes=[512, 256, 128, 64],
        learning_rate=0.0005,
        epsilon=0.9,
        epsilon_decay=0.9995,
        epsilon_min=0.05,
        batch_size=64,
        memory_size=50000
    )
    
    # Crear oponentes variados
    oponentes = [
        HeuristicAgent(aggression=0.3),
        RandomAgent(weights=[0.2, 0.5, 0.3]),
        ConservativeAgent()
    ]
    print(f"ğŸ¤– Oponentes creados: {len(oponentes)} bots")
    
    # Crear entrenador
    trainer = PokerNeuralTrainer(env, agent, oponentes)
    
    # Intentar cargar modelo previo
    if agent.load_model("poker_holdem_dqn.pth"):
        print("ğŸ“‚ Modelo previo cargado, continuando entrenamiento...")
    else:
        print("ğŸ†• Iniciando entrenamiento desde cero...")
    
    # Entrenar
    print(f"ğŸš€ Iniciando entrenamiento por {n_episodios} episodios...")
    start_time = time.time()
    
    historia_recompensas = []
    
    for episodio in range(n_episodios):
        recompensa = trainer.train_episode()
        historia_recompensas.append(recompensa)
        
        # Progreso cada cierto nÃºmero de episodios
        if episodio % 50 == 0:
            avg_reward = np.mean(historia_recompensas[-50:]) if historia_recompensas else 0
            elapsed = time.time() - start_time
            print(f"ğŸ“ˆ Episodio {episodio:4d} | Recompensa avg: {avg_reward:6.2f} | "
                  f"Epsilon: {agent.epsilon:.3f} | Tiempo: {elapsed:.1f}s")
        
        # Guardar modelo periÃ³dicamente
        if episodio % guardar_cada == 0 and episodio > 0:
            agent.save_model(f"poker_holdem_dqn_ep{episodio}.pth")
    
    # Guardar modelo final
    agent.save_model("poker_holdem_dqn_final.pth")
    
    # EstadÃ­sticas finales
    tiempo_total = time.time() - start_time
    agent.print_stats()
    print(f"\nâ±ï¸ Tiempo total de entrenamiento: {tiempo_total:.1f} segundos")
    print(f"âš¡ Episodios por segundo: {n_episodios/tiempo_total:.2f}")
    
    return agent, historia_recompensas


def entrenar_neural_kuhn(n_episodios=5000):
    """
    Entrenamiento rÃ¡pido en Kuhn Poker (mÃ¡s simple)
    """
    print("ğŸ¯" + "="*50 + "ğŸ¯")
    print("    ENTRENAMIENTO NEURAL - KUHN POKER")
    print("ğŸ¯" + "="*50 + "ğŸ¯")
    
    # Crear entorno simple
    env = KuhnEnv()
    
    # Crear agente neural mÃ¡s pequeÃ±o para Kuhn
    agent = NeuralPokerAgent(
        input_size=20,
        hidden_sizes=[128, 64, 32],
        learning_rate=0.001,
        epsilon=0.8,
        epsilon_decay=0.999,
        batch_size=32,
        memory_size=10000
    )
    
    # Oponente simple para Kuhn
    oponente = RandomAgent()
    
    print("ğŸš€ Iniciando entrenamiento Kuhn...")
    start_time = time.time()
    
    historia_recompensas = []
    
    for episodio in range(n_episodios):
        try:
            obs, _ = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                current_player = env.current_player
                
                if current_player == 0:  # Agente neural
                    action = agent.action(obs)
                    prev_obs = obs
                else:  # Oponente
                    action = oponente.action(obs)
                
                next_obs, rewards, done, info = env.step(action)
                
                if current_player == 0:
                    reward = rewards[0] if isinstance(rewards, tuple) else 0
                    agent.update(prev_obs, action, reward, next_obs, done)
                    total_reward += reward
                
                obs = next_obs
            
            historia_recompensas.append(total_reward)
            
            # Progreso
            if episodio % 500 == 0:
                avg_reward = np.mean(historia_recompensas[-500:])
                print(f"ğŸ“ˆ Episodio {episodio:4d} | Recompensa avg: {avg_reward:6.2f} | "
                      f"Epsilon: {agent.epsilon:.3f}")
        
        except Exception as e:
            print(f"âš ï¸ Error en episodio {episodio}: {e}")
            continue
    
    # Guardar modelo
    agent.save_model("poker_kuhn_dqn.pth")
    
    tiempo_total = time.time() - start_time
    print(f"\nâ±ï¸ Entrenamiento Kuhn completado en {tiempo_total:.1f}s")
    
    return agent, historia_recompensas


def evaluar_agente(agente, n_partidas=100, env_type="holdem"):
    """
    Evaluar el rendimiento del agente entrenado
    """
    print(f"\nğŸ§ª Evaluando agente en {n_partidas} partidas ({env_type})...")
    
    # Crear entorno
    if env_type == "holdem":
        env = HoldemEnv(n_players=4)
        oponentes = [HeuristicAgent(), RandomAgent(), ConservativeAgent()]
    else:
        env = KuhnEnv()
        oponentes = [RandomAgent()]
    
    # Desactivar exploraciÃ³n para evaluaciÃ³n
    epsilon_original = agente.epsilon
    agente.epsilon = 0.0  # Solo explotaciÃ³n
    
    victorias = 0
    recompensas = []
    
    for partida in range(n_partidas):
        try:
            obs, _ = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                current_player = env.current_player
                
                if current_player == 0:  # Nuestro agente
                    action = agente.action(obs)
                else:  # Oponentes
                    if env_type == "holdem" and current_player - 1 < len(oponentes):
                        action = oponentes[current_player - 1].action(obs)
                    else:
                        action = random.choice([0, 1, 2])
                
                obs, rewards, done, info = env.step(current_player, action)
                
                if current_player == 0:
                    reward = rewards[0] if isinstance(rewards, (list, tuple)) else 0
                    total_reward += reward
            
            recompensas.append(total_reward)
            if total_reward > 0:
                victorias += 1
        
        except Exception as e:
            print(f"âš ï¸ Error en evaluaciÃ³n partida {partida}: {e}")
            continue
    
    # Restaurar epsilon
    agente.epsilon = epsilon_original
    
    # Resultados
    tasa_victoria = (victorias / n_partidas) * 100
    recompensa_promedio = np.mean(recompensas)
    
    print(f"ğŸ“Š Resultados de evaluaciÃ³n:")
    print(f"   ğŸ† Victorias: {victorias}/{n_partidas} ({tasa_victoria:.1f}%)")
    print(f"   ğŸ’° Recompensa promedio: {recompensa_promedio:.2f}")
    print(f"   ğŸ“ˆ Recompensa mÃ¡xima: {max(recompensas):.2f}")
    print(f"   ğŸ“‰ Recompensa mÃ­nima: {min(recompensas):.2f}")
    
    return tasa_victoria, recompensa_promedio


def graficar_progreso(historia_recompensas, titulo="Progreso de Entrenamiento"):
    """
    Crear grÃ¡fico del progreso de entrenamiento
    """
    try:
        plt.figure(figsize=(12, 6))
        
        # Suavizar curva con media mÃ³vil
        window = min(50, len(historia_recompensas) // 10)
        if window > 1:
            recompensas_suaves = np.convolve(historia_recompensas, 
                                           np.ones(window)/window, mode='valid')
            plt.plot(recompensas_suaves, label=f'Media mÃ³vil ({window} episodios)', linewidth=2)
        
        plt.plot(historia_recompensas, alpha=0.3, label='Recompensas por episodio')
        plt.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Break-even')
        
        plt.title(titulo)
        plt.xlabel('Episodio')
        plt.ylabel('Recompensa')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Guardar grÃ¡fico
        plt.savefig('entrenamiento_progreso.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š GrÃ¡fico guardado como 'entrenamiento_progreso.png'")
    
    except Exception as e:
        print(f"âš ï¸ Error creando grÃ¡fico: {e}")


def main():
    """
    FunciÃ³n principal de entrenamiento
    """
    print("ğŸ§ " + "="*70 + "ğŸ§ ")
    print("    ENTRENAMIENTO DE RED NEURONAL PARA POKER")
    print("             CardBlack Neural Poker AI")
    print("ğŸ§ " + "="*70 + "ğŸ§ ")
    
    # Verificar dependencias
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__} detectado")
        print(f"ğŸ”¥ CUDA disponible: {torch.cuda.is_available()}")
    except ImportError:
        print("âŒ PyTorch no instalado. Instala con: pip install torch")
        return
    
    # MenÃº de opciones
    print("\nğŸ¯ Opciones de entrenamiento:")
    print("1. ğŸ° Entrenar en Texas Hold'em (complejo, lento)")
    print("2. ğŸ¯ Entrenar en Kuhn Poker (simple, rÃ¡pido)")
    print("3. ğŸ§ª Solo probar agente neural")
    print("4. ğŸ“Š Evaluar modelo existente")
    
    try:
        opcion = input("\nğŸ‘‰ Selecciona una opciÃ³n (1-4): ").strip()
        
        if opcion == "1":
            # Entrenamiento Hold'em
            agente, historia = entrenar_neural_holdem(n_episodios=1000)
            graficar_progreso(historia, "Entrenamiento Texas Hold'em")
            evaluar_agente(agente, n_partidas=50, env_type="holdem")
        
        elif opcion == "2":
            # Entrenamiento Kuhn
            agente, historia = entrenar_neural_kuhn(n_episodios=3000)
            graficar_progreso(historia, "Entrenamiento Kuhn Poker")
            evaluar_agente(agente, n_partidas=100, env_type="kuhn")
        
        elif opcion == "3":
            # Solo testing
            from neural_agent import test_neural_agent
            test_neural_agent()
        
        elif opcion == "4":
            # Evaluar modelo existente
            agente = NeuralPokerAgent()
            if agente.load_model("poker_holdem_dqn_final.pth"):
                evaluar_agente(agente, n_partidas=100, env_type="holdem")
            elif agente.load_model("poker_kuhn_dqn.pth"):
                evaluar_agente(agente, n_partidas=200, env_type="kuhn")
            else:
                print("âŒ No se encontrÃ³ modelo entrenado")
        
        else:
            print("âŒ OpciÃ³n invÃ¡lida")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Entrenamiento interrumpido por el usuario")
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    print("\nğŸ‰ Proceso completado!")


def entrenamiento_comparativo():
    """
    Comparar rendimiento entre diferentes agentes
    """
    print("âš”ï¸ BATALLA DE AGENTES âš”ï¸")
    
    # Crear agentes
    agentes = {
        "Neural": NeuralPokerAgent(),
        "Heuristic": HeuristicAgent(),
        "Random": RandomAgent(),
        "Conservative": ConservativeAgent(),
        "Aggressive": AggressiveAgent()
    }
    
    # Cargar modelo neural si existe
    agentes["Neural"].load_model("poker_holdem_dqn_final.pth")
    agentes["Neural"].epsilon = 0.0  # Sin exploraciÃ³n para evaluaciÃ³n
    
    # Crear entorno
    env = HoldemEnv(n_players=4)
    
    resultados = {nombre: [] for nombre in agentes.keys()}
    n_partidas = 100
    
    print(f"ğŸ® Simulando {n_partidas} partidas...")
    
    for partida in range(n_partidas):
        try:
            obs, _ = env.reset()
            done = False
            
            while not done:
                current_player = env.current_player
                
                # Seleccionar agente para el jugador actual
                if current_player == 0:
                    agente_actual = agentes["Neural"]
                elif current_player == 1:
                    agente_actual = agentes["Heuristic"]
                elif current_player == 2:
                    agente_actual = agentes["Random"]
                else:
                    agente_actual = agentes["Conservative"]
                
                action = agente_actual.action(obs)
                obs, rewards, done, info = env.step(current_player, action)
            
            # Registrar resultados
            nombres_agentes = ["Neural", "Heuristic", "Random", "Conservative"]
            for i, nombre in enumerate(nombres_agentes):
                if i < len(rewards):
                    resultados[nombre].append(rewards[i])
        
        except Exception as e:
            print(f"âš ï¸ Error en partida {partida}: {e}")
            continue
    
    # Mostrar resultados
    print("\nğŸ† RESULTADOS FINALES:")
    for nombre, recompensas in resultados.items():
        if recompensas:
            promedio = np.mean(recompensas)
            victorias = sum(1 for r in recompensas if r > 0)
            tasa_victoria = (victorias / len(recompensas)) * 100
            print(f"   {nombre:12} | Promedio: {promedio:6.2f} | Victorias: {tasa_victoria:5.1f}%")


if __name__ == "__main__":
    # Verificar si se ejecuta directamente
    if len(sys.argv) > 1:
        if sys.argv[1] == "compare":
            entrenamiento_comparativo()
        elif sys.argv[1] == "kuhn":
            entrenar_neural_kuhn()
        elif sys.argv[1] == "holdem":
            entrenar_neural_holdem()
        else:
            print("Opciones: python train_neural_poker.py [compare|kuhn|holdem]")
    else:
        main()
